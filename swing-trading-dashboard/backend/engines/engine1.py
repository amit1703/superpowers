"""
Engine 1: Battlefield Mapper — S/R Infrastructure
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Method:
  1. Resample daily OHLCV → weekly.
  2. Collect weekly closes + weekly pivot highs/lows.
  3. Apply Kernel Density Estimation (scipy gaussian_kde) on the
     combined price-point cloud to find institutional clustering.
  4. Extract local density peaks → significant S/R price levels.
  5. Convert each peak into a ZONE:  level ± (0.2 × Daily ATR).
  6. Merge peaks that are within 1 ATR of each other (remove duplicates).
  7. Classify zones as SUPPORT (below current price) or RESISTANCE (above).
"""

import os
import sys
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
import yfinance as yf
from scipy.signal import argrelextrema
from scipy.stats import gaussian_kde

sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from indicators import atr as _atr


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

def calculate_sr_zones(
    ticker: str,
    df: Optional[pd.DataFrame] = None,
) -> List[Dict]:
    """
    Parameters
    ----------
    ticker : str
        Stock ticker (used for fetching if df is None).
    df : pd.DataFrame, optional
        Pre-fetched daily OHLCV with columns including 'Adj Close',
        'High', 'Low'.  If None the function downloads 2 years of data.

    Returns
    -------
    list of dicts
        Each dict: {level, upper, lower, type, atr}
        Sorted ascending by level.
    """
    try:
        data = _load(ticker, df)
        if data is None or len(data) < 60:
            return []

        adj_col = _adj_col(data)
        atr_series = _atr(data["High"], data["Low"], data[adj_col], length=14)

        if atr_series.dropna().empty:
            return []

        daily_atr = float(atr_series.dropna().iloc[-1])
        if daily_atr <= 0:
            return []

        zone_half_width = 0.2 * daily_atr

        # ── Weekly resample ──────────────────────────────────────────────
        weekly = (
            data.resample("W")
            .agg({adj_col: "last", "High": "max", "Low": "min"})
            .dropna()
        )

        if len(weekly) < 10:
            return []

        # ── Pivot highs / lows (adaptive window) ─────────────────────────
        order = max(2, len(weekly) // 20)
        hi = weekly["High"].values
        lo = weekly["Low"].values
        cl = weekly[adj_col].values

        ph_idx = argrelextrema(hi, np.greater_equal, order=order)[0]
        pl_idx = argrelextrema(lo, np.less_equal, order=order)[0]

        price_points = np.concatenate([cl, hi[ph_idx], lo[pl_idx]])
        price_points = price_points[~np.isnan(price_points) & (price_points > 0)]

        if len(price_points) < 10:
            return []

        # ── KDE ────────────────────────────────────────────────────────────
        kde = gaussian_kde(price_points, bw_method="scott")
        p_min = price_points.min() * 0.98
        p_max = price_points.max() * 1.02
        x = np.linspace(p_min, p_max, 600)
        density = kde(x)

        peak_idx = argrelextrema(density, np.greater, order=8)[0]
        if len(peak_idx) == 0:
            return []

        peak_prices = x[peak_idx]
        peak_densities = density[peak_idx]

        # Keep only statistically significant peaks (top 70 %)
        threshold = np.percentile(peak_densities, 30)
        peak_prices = peak_prices[peak_densities >= threshold]

        # ── Merge nearby peaks (within 1 ATR) ────────────────────────────
        merged: List[float] = []
        cluster: List[float] = [float(np.sort(peak_prices)[0])]
        for p in np.sort(peak_prices)[1:]:
            if p - cluster[-1] < daily_atr:
                cluster.append(p)
            else:
                merged.append(float(np.mean(cluster)))
                cluster = [p]
        merged.append(float(np.mean(cluster)))

        # ── Build zone dicts ─────────────────────────────────────────────
        current_price = float(data[adj_col].iloc[-1])
        zones: List[Dict] = []

        for level in merged:
            zone_type = "RESISTANCE" if level > current_price else "SUPPORT"
            zones.append(
                {
                    "level": round(level, 2),
                    "upper": round(level + zone_half_width, 2),
                    "lower": round(level - zone_half_width, 2),
                    "type": zone_type,
                    "atr": round(daily_atr, 2),
                }
            )

        zones.sort(key=lambda z: z["level"])
        return zones

    except Exception as exc:  # noqa: BLE001
        print(f"[Engine1] {ticker}: {exc}")
        return []


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _load(ticker: str, df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:
    if df is not None:
        data = df.copy()
    else:
        data = yf.download(
            ticker,
            period="2y",
            interval="1d",
            auto_adjust=False,
            prepost=False,
            progress=False,
            threads=False,
        )

    if data.empty:
        return None

    if isinstance(data.columns, pd.MultiIndex):
        data.columns = data.columns.get_level_values(0)

    return data


def _adj_col(df: pd.DataFrame) -> str:
    return "Adj Close" if "Adj Close" in df.columns else "Close"
